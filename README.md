# ğŸ§ whisperpyannote â€” Audio & Video Transcription + Speaker Diarization

![License](https://img.shields.io/badge/License-MIT-green)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Whisper](https://img.shields.io/badge/Whisper-STT-orange)
![Pyannote](https://img.shields.io/badge/Pyannote-Diarization-purple)
![Status](https://img.shields.io/badge/Status-Active-success)
![PRs](https://img.shields.io/badge/PRs-Welcome-brightgreen)

**whisperpyannote** is a Python script that performs:

- ğŸ“ Automatic speech transcription  
- ğŸ—£ï¸ Speaker diarization (who speaks when)  
- ğŸ¥ on both audio and video files  

It combines **Whisper (OpenAI)** for transcription and **Pyannote Audio** for speaker identification.

ğŸ”— Whisper â†’ https://github.com/openai/whisper  
ğŸ”— Pyannote community diarization model â†’ https://huggingface.co/pyannote/speaker-diarization-community-1  

---

## ğŸ™ Acknowledgements

This project uses two major open-source components:

- **Whisper (OpenAI)** â€” MIT License  
- **Pyannote Audio** and the model **speaker-diarization-community-1** â€” MIT License  

Thanks to their authors, maintainers, and contributors for making high-quality open models available to everyone.

---

## âœ¨ Features

- ğŸ¥ Automatic audio extraction from videos *(requires FFmpeg)*  
- ğŸ”„ Conversion to mono 16 kHz *(requires FFmpeg)*  
- ğŸ“ Whisper transcription  
- ğŸ—£ï¸ Pyannote diarization  
- ğŸ§  Smart merging of segments  
- â³ Speaking time per speaker
- ğŸ¬ Optional subtitle export (**SRT / VTT**)  
- ğŸ“„ Optional structured output (**JSON**)  

---

## ğŸ–¥ï¸ Graphical User Interface

A desktop GUI is available.  
See ğŸ‘‰ [GUI.md](./GUI.md)

---

## âš™ï¸ System Requirement

**FFmpeg is required** and must be available in your system PATH.

The script relies on FFmpeg to:
- Extract audio from video files
- Convert audio to mono 16 kHz WAV

> Audio/video inputs are automatically converted when needed.

### Install FFmpeg

- **macOS (Homebrew)**  
  ```
  brew install ffmpeg
  ```

- **Ubuntu / Debian**  
  ```
  sudo apt install ffmpeg
  ```

- **Windows**  
  Download from https://ffmpeg.org/download.html  
  and make sure `ffmpeg` is added to your PATH.

- **Windows (via terminal â€“ winget)**
  ```
  winget install Gyan.FFmpeg
  ```
  Then restart your terminal and verify:
  ```
  ffmpeg -version
  ```
---

# ğŸ™ï¸ Recording with OBS (recommended)

Steps:

1. Install OBS: https://obsproject.com/  
2. Add **Display Capture** or **Window Capture**  
3. Add **Audio Input Capture** (microphone)  
4. Optional: capture system audio  
   - macOS â†’ install **BlackHole** (https://existential.audio/blackhole/)  
   - Windows â†’ enable **Stereo Mix** or use **VB-Cable**  
5. Record in MP4 or MKV  
6. Use the recorded file with `whisperpyannote`

OBS recordings (.mp4, .mov, .mkv) work perfectly.

---

## ğŸš€ Installation

### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/nantaidsl95/whisperpyannote.git
cd whisperpyannote
```

### 2ï¸âƒ£ Install FFmpeg (required)

### 3ï¸âƒ£ Create a virtual environment

**macOS / Linux**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows (PowerShell)**
```powershell
python -m venv venv
venv\Scripts\Activate.ps1
```
> âš ï¸ If you get a PowerShell execution policy error, run PowerShell **as Administrator** and execute:
> ```powershell
> Set-ExecutionPolicy RemoteSigned
> ```
> Then reopen your terminal and activate the virtual environment again.

**Windows (Command Prompt / cmd.exe)**
```cmd
python -m venv venv
venv\Scripts\activate.bat
```

### 4ï¸âƒ£ Install Python dependencies
```
pip install -r requirements.txt
```

---

## ğŸ”‘ Hugging Face Token (required for Pyannote)

Diarization uses the model:

```
pyannote/speaker-diarization-community-1
```

Speaker diarization relies on the Hugging Face model:

- **pyannote/speaker-diarization-community-1**  
  https://huggingface.co/pyannote/speaker-diarization-community-1

To use this model, you must complete **all** of the following steps:

1. **Accept the model terms**  
   Visit the model page and accept its usage conditions:  
   https://huggingface.co/pyannote/speaker-diarization-community-1

2. **Create a Hugging Face access token**  
   Go to the token settings page:  
   https://huggingface.co/settings/tokens  

   Create a new token with **Read** permissions.

3. **Export the token as an environment variable (recommended)**  

   **macOS / Linux**
   ```
   export HF_TOKEN="your_token_here"
   ```

   **Windows (PowerShell)**
   ```
   $env:HF_TOKEN="your_token_here"
   ```

   Using an environment variable is the safest method and avoids exposing the token in command history or scripts.

4. **Alternative methods (supported but not recommended)**  
   The token can also be provided via:
   - the `--hf_token` CLI option  
   - the interactive prompt (`--ask_token`)  

   Exporting the token remains the preferred approach.
---

# ğŸ› ï¸ Full CLI Options

### Required arguments

| Argument | Description |
|---------|-------------|
| `input_path` | Audio/video file to process |
| `output_file` | Output text file |

---

### Transcription & diarization options

| Option | Description | Values |
|--------|-------------|--------|
| `--whisper_model` | Whisper model | tiny, base, small, medium, large, turbo |
| `--language` | Force transcription language | en, fr, deâ€¦ |

---

### Exclusive modes

| Option | Description |
|--------|-------------|
| `--transcription_only` | Only transcription |
| `--diarization_only` | Only diarization |

---

### Token management

| Option | Description |
|--------|-------------|
| `--hf_token` | Provide HF token directly |
| `--ask_token` | Force interactive prompt |

Also detected automatically:
- `HF_TOKEN`
- `HUGGINGFACE_TOKEN`

---

### Output formats

| Option | Description |
|--------|-------------|
| `--json` | Write a JSON file alongside the text output |
| `--srt` | Generate SRT subtitles |
| `--vtt` | Generate VTT subtitles |
| `--subs_no_speaker` | Do not prefix subtitles with speaker labels |

Subtitles behavior:
- Transcription only â†’ Whisper-based subtitles
- Full mode â†’ speaker-merged subtitles
- Diarization only â†’ no subtitles (no text)

---

### Temporary files

| Option | Description |
|--------|-------------|
| `--keep_temp` | Keep temporary WAV files |

---

# ğŸš€ Usage Examples

```
python whisperpyannote.py input.mp4 output.txt
python whisperpyannote.py audio.wav output.txt --transcription_only
python whisperpyannote.py audio.wav output.txt --diarization_only
python whisperpyannote.py audio.wav output.txt --whisper_model medium
python whisperpyannote.py audio.wav output.txt --language fr
python whisperpyannote.py input.mp4 output.txt --srt --vtt
```

---

## ğŸ“œ Example Output

```
â³ Speaking time per speaker:
SPEAKER_00: 00:12:34
SPEAKER_01: 00:08:45

[00:00:01â€“00:00:05] SPEAKER_00: Hello everyone!
[00:00:06â€“00:00:10] SPEAKER_01: Hi, how are you?
```

---

## âš ï¸ Known Limitations

- Diarization accuracy may decrease with overlapping speakers
- Some segments may be assigned to an `unknown` speaker
- Whisper segmentation depends on the selected model

---

## ğŸ“ Project Structure

```
whisperpyannote/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ GUI.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_gui.txt
â”œâ”€â”€ whisperpyannote.py
â””â”€â”€ whisperpyannote_gui.py
```
---

## ğŸ“„ License

This project is distributed under the MIT License.  
See the [LICENSE](./LICENSE) file for details.

---

## ğŸ‘¤ Author

Developed by **Marc Delage**  
GitHub â†’ https://github.com/nantaidsl95
