# ğŸ§ whisperpyannote â€” Audio & Video Transcription + Speaker Diarization

![License](https://img.shields.io/badge/License-MIT-green)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Whisper](https://img.shields.io/badge/Whisper-STT-orange)
![Pyannote](https://img.shields.io/badge/Pyannote-Diarization-purple)
![Status](https://img.shields.io/badge/Status-Active-success)
![PRs](https://img.shields.io/badge/PRs-Welcome-brightgreen)

**whisperpyannote** is a Python script that performs:

- ğŸ“ automatic speech transcription  
- ğŸ—£ï¸ speaker diarization (who speaks when)  
- ğŸ¥ on both audio and video files  

It combines **Whisper (OpenAI)** for transcription and **Pyannote Audio** for speaker identification.

ğŸ”— Whisper â†’ https://github.com/openai/whisper  
ğŸ”— Pyannote community diarization model â†’ https://huggingface.co/pyannote/speaker-diarization-community-1  

---

## ğŸ™ Acknowledgements

This project uses two major open-source components:

- **Whisper (OpenAI)** â€” MIT License  
- **Pyannote Audio** and the model **speaker-diarization-community-1** â€” MIT License  

Thanks to their authors, maintainers, and contributors for making high-quality open models available to everyone.

---

## âœ¨ Features

- ğŸ¥ Automatic audio extraction from videos *(requires FFmpeg)*  
- ğŸ”„ Conversion to mono 16 kHz *(requires FFmpeg)*  
- ğŸ“ Whisper transcription  
- ğŸ—£ï¸ Pyannote diarization  
- ğŸ§  Smart merging of segments  
- â³ Speaking time per speaker
- ğŸ¬ Optional subtitle export (**SRT / VTT**)  
- ğŸ“„ Optional structured output (**JSON**)  

---

## âš™ï¸ System Requirement

**FFmpeg is required** and must be available in your system PATH.

The script relies on FFmpeg to:
- extract audio from video files
- convert audio to mono 16 kHz WAV

> Audio/video inputs are automatically converted when needed.

### Install FFmpeg

- **macOS (Homebrew)**  
  ```
  brew install ffmpeg
  ```

- **Ubuntu / Debian**  
  ```
  sudo apt install ffmpeg
  ```

- **Windows**  
  Download from https://ffmpeg.org/download.html  
  and make sure `ffmpeg` is added to your PATH.

- **Windows (via terminal â€“ winget)**
  ```
  winget install Gyan.FFmpeg
  ```
  Then restart your terminal and verify:
  ```
  ffmpeg -version
  ```
---

# ğŸ™ï¸ Recording with OBS (recommended)

Steps:

1. Install OBS: https://obsproject.com/  
2. Add **Display Capture** or **Window Capture**  
3. Add **Audio Input Capture** (microphone)  
4. Optional: capture system audio  
   - macOS â†’ install **BlackHole** (https://existential.audio/blackhole/)  
   - Windows â†’ enable **Stereo Mix** or use **VB-Cable**  
5. Record in MP4 or MKV  
6. Use the recorded file with `whisperpyannote`

OBS recordings (.mp4, .mov, .mkv) work perfectly.

---

## ğŸš€ Installation

### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/nantaidsl95/whisperpyannote.git
cd whisperpyannote
```

### 2ï¸âƒ£ Install FFmpeg (required)

### 3ï¸âƒ£ Create a virtual environment
```
python3 -m venv venv
source venv/bin/activate
```

### 4ï¸âƒ£ Install Python dependencies
```
pip install -r requirements.txt
```

---

## ğŸ”‘ Hugging Face Token (required for Pyannote)

Diarization uses the model:

```
pyannote/speaker-diarization-community-1
```

Access to this model requires:
- accepting its conditions on Hugging Face
- a valid Hugging Face access token

The token can be provided via:
- environment variables `HF_TOKEN` or `HUGGINGFACE_TOKEN`
- CLI option `--hf_token`
- interactive prompt (when running in a terminal)

---

# ğŸ› ï¸ Full CLI Options

### Required arguments

| Argument | Description |
|---------|-------------|
| `input_path` | Audio/video file to process |
| `output_file` | Output text file |

---

### Transcription & diarization options

| Option | Description | Values |
|--------|-------------|--------|
| `--whisper_model` | Whisper model | tiny, base, small, medium, large, turbo |
| `--language` | Force transcription language | en, fr, deâ€¦ |

---

### Exclusive modes

| Option | Description |
|--------|-------------|
| `--transcription_only` | Only transcription |
| `--diarization_only` | Only diarization |

---

### Token management

| Option | Description |
|--------|-------------|
| `--hf_token` | Provide HF token directly |
| `--ask_token` | Force interactive prompt |

Also detected automatically:
- `HF_TOKEN`
- `HUGGINGFACE_TOKEN`

---

### Output formats

| Option | Description |
|--------|-------------|
| `--json` | Write a JSON file alongside the text output |
| `--srt` | Generate SRT subtitles |
| `--vtt` | Generate VTT subtitles |
| `--subs_no_speaker` | Do not prefix subtitles with speaker labels |

Subtitles behavior:
- transcription only â†’ Whisper-based subtitles
- full mode â†’ speaker-merged subtitles
- diarization only â†’ no subtitles (no text)

---

### Temporary files

| Option | Description |
|--------|-------------|
| `--keep_temp` | Keep temporary WAV files |

---

# ğŸš€ Usage Examples

```
python whisperpyannote.py input.mp4 output.txt
python whisperpyannote.py audio.wav output.txt --transcription_only
python whisperpyannote.py audio.wav output.txt --diarization_only
python whisperpyannote.py audio.wav output.txt --whisper_model medium
python whisperpyannote.py audio.wav output.txt --language fr
python whisperpyannote.py input.mp4 output.txt --srt --vtt
```

---

## ğŸ“œ Example Output

```
â³ Speaking time per speaker:
SPEAKER_00: 00:12:34
SPEAKER_01: 00:08:45

[00:00:01â€“00:00:05] SPEAKER_00: Hello everyone!
[00:00:06â€“00:00:10] SPEAKER_01: Hi, how are you?
```

---

## âš ï¸ Known Limitations

- diarization accuracy may decrease with overlapping speakers
- some segments may be assigned to an `unknown` speaker
- Whisper segmentation depends on the selected model

---

## ğŸ“ Project Structure

```
whisperpyannote/
â”œâ”€â”€ whisperpyannote.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

---

## ğŸ“„ License

This project is distributed under the MIT License.  
See the [LICENSE](./LICENSE) file for details.

---

## ğŸ‘¤ Author

Developed by **Marc Delage**  
GitHub â†’ https://github.com/nantaidsl95
